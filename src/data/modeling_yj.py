#%%
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import KFold
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
import time
from datetime import datetime
from xgboost import XGBRegressor
import lightgbm as lgb

# set kr font
import matplotlib.font_manager
fontlist = [font.name for font in matplotlib.font_manager.fontManager.ttflist]
# print(fontlist)
krfont = ['Malgun Gothic', 'NanumGothic', 'AppleGothic']
for font in krfont:
    if font in fontlist:
        mpl.rc('font', family=font)

#%%
# train ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°

data_dir = '../../data/processed/cleaned_data'
os.makedirs(data_dir, exist_ok=True)

traindata_filename = 'train_clean.csv'
testdata_filename = 'test_clean.csv'

traindata_path = os.path.join(data_dir, traindata_filename)
testdata_path = os.path.join(data_dir, testdata_filename)

df = pd.read_csv(traindata_path)
df_test = pd.read_csv(testdata_path)
# ë°ì´í„° í™•ì¸
print(df.info())
print(df_test.info())

#%%
# ë²”ì£¼í˜•, ìˆ˜ì¹˜í˜• feature ë¶„ë¦¬
obj_cols = df.select_dtypes(include='object').columns.tolist()
num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

print(obj_cols)
print(num_cols)

#%%
# ì›ë³¸ df ë³µì‚¬í•˜ì—¬ ì¸ì½”ë”© ì²˜ë¦¬í•  df ìƒì„±
df_encoded = df.copy()
df_test_encoded = df_test.copy()

#%%
# OneHotEncoding => ì§€ì—­êµ¬, ë¸Œëœë“œë“±ê¸‰ì„ ì¸ì½”ë”© í•˜ë ¤ í–ˆìœ¼ë‚˜ ë³€ìˆ˜ì¤‘ìš”ë„ ì¸¡ì •ì´ ì˜ëª»ë˜ëŠ” ê²ƒ ê°™ì•„ ì§€ì—­êµ¬ëŠ” drop
def one_hot_encoding_with_dataset(df_train_org, df_test_org, encoding_cols):
    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)

    # train ë°ì´í„° fit
    encoder.fit(df_train_org[encoding_cols])

    # transform train/test
    train_ohe = encoder.transform(df_train_org[encoding_cols])
    test_ohe = encoder.transform(df_test_org[encoding_cols])

    # ê²°ê³¼ëŠ” numpy ë°°ì—´ì´ë¯€ë¡œ, ì»¬ëŸ¼ëª… ë¶™ì´ê¸° (get_feature_names_out)
    ohe_columns = encoder.get_feature_names_out(encoding_cols)

    train_ohe_df = pd.DataFrame(train_ohe, columns=ohe_columns, index=df_train_org.index)
    test_ohe_df = pd.DataFrame(test_ohe, columns=ohe_columns, index=df_test_org.index)

    # ê¸°ì¡´ ìˆ«ìí˜• í”¼ì²˜ë‘ í•©ì¹˜ê¸° ì˜ˆì‹œ
    train_final = pd.concat([df_train_org.drop(columns=encoding_cols), train_ohe_df], axis=1)
    test_final = pd.concat([df_test_org.drop(columns=encoding_cols), test_ohe_df], axis=1)

    return train_final, test_final

#%%
# # OneHotEncoding ì ìš© (ë¸Œëœë“œë“±ê¸‰)
df_encoded, df_test_encoded = one_hot_encoding_with_dataset(df_encoded, df_test_encoded, ['ë¸Œëœë“œë“±ê¸‰'])

#%%
# print(df_encoded.info())
# print(df_test_encoded.info())

#%%
# ë¡œê·¸ ë³€í™˜ (log1p) ì ìš©
df_encoded["log_target"] = np.log1p(df["target"])


#%%
# í”¼ì²˜ ì„ íƒ
selected_features = [
    'ê³„ì•½ë…„ë„',
    'ê³„ì•½ì›”',
    'ê°•ë‚¨3êµ¬ì—¬ë¶€',
    'ì „ìš©ë©´ì ',
    'ì¸µ',
    'ê±´ì¶•ë…„ë„',
    'í™ˆí˜ì´ì§€ìœ ë¬´',
    'ì‚¬ìš©í—ˆê°€ì—¬ë¶€',
    'ì•„íŒŒíŠ¸ì´ë¦„ê¸¸ì´',
    'ì¢Œí‘œX',
    'ì¢Œí‘œY',
    'ì§€í•˜ì² ìµœë‹¨ê±°ë¦¬',
    'ë°˜ê²½_500m_ì§€í•˜ì² ì—­_ìˆ˜',
    'ë²„ìŠ¤ìµœë‹¨ê±°ë¦¬',
    'ë°˜ê²½_500m_ë²„ìŠ¤ì •ë¥˜ì¥_ìˆ˜',
    'ë¸Œëœë“œë“±ê¸‰_ê¸°íƒ€',
    'ë¸Œëœë“œë“±ê¸‰_í”„ë¦¬ë¯¸ì—„',
    'ë¸Œëœë“œë“±ê¸‰_í•˜ì´ì—”ë“œ'
]

# print(selected_features)

#%%
# df_encoded.columns

#%%

# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìƒê´€ê´€ê³„
selected_num_cols = df_encoded[selected_features].select_dtypes(include=['int64', 'float64']).columns.tolist()
print(selected_num_cols)

df_num = df_encoded[selected_num_cols]
corr_matrix = df_num.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title('Correlation Heatmap of Selected Columns')
plt.show()

#%%
seed = 42

#%%
df_modeling = df_encoded.copy()

# train ë°ì´í„°ì…‹ feature, target ë¶„ë¦¬
X = df_modeling[selected_features]
y = df_modeling['log_target']

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
X_test = df_test_encoded[selected_features]

#%%

# ë°ì´í„°ì…‹
print(X.info())
print(y.info())
print(X_test.info())

#%%

def get_next_dir_number(base_path="results"):
    os.makedirs(base_path, exist_ok=True)
    print(f'test :: base path > {base_path}')
    existing = [d for d in os.listdir(base_path) if d.startswith("modeling_")]
    print(f'existing : {existing}')
    numbers = []
    for name in existing:
        try:
            number = int(name.split("_")[1])
            numbers.append(number)
        except:
            pass
    next_number = max(numbers, default=0) + 1

    return f'{base_path}/modeling_{next_number:03d}'

#%%
def show_feature_importances(model, name, selected_features):
    importances = pd.Series(model.feature_importances_, index=selected_features)
    importances_sorted = importances.sort_values(ascending=False)[:20]

    plt.figure(figsize=(10, 6))
    sns.barplot(x=importances_sorted.values, y=importances_sorted.index)
    plt.title(f'{name} - Feature Importances (Top 20)')
    plt.tight_layout()
    plt.show()

#%%
def model_training(is_submission,
                   model, # í•™ìŠµí•  ëª¨ë¸
                   model_name, # í•™ìŠµí•  ëª¨ë¸ëª…
                   selected_features, # í•™ìŠµì— ì‚¬ìš©í•œ features
                   X, # í•™ìŠµ ë°ì´í„°ì…‹ (features) (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë¶„í• )
                   y, # í•™ìŠµ ë°ì´í„° ì •ë‹µ(target) (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë¶„í• )
                   X_test=None, # ì œì¶œìš© test ë°ì´í„°ì…‹ (features)
                   show_feature_importance = False): # ë³€ìˆ˜ ì¤‘ìš”ë„ ì‹œê°í™” í‘œì‹œì—¬ë¶€
    
    basepath_dir = f"../../../output"
    output_dir = get_next_dir_number(basepath_dir)
    os.makedirs(output_dir, exist_ok=True)  # ê²½ë¡œ ì—†ìœ¼ë©´ ìƒì„±
    print(f"ğŸ”¹ modeling directory: {output_dir}")

    features_filename = 'selected_features.txt'
    features_filepath = os.path.join(output_dir, features_filename)
    
    with open(features_filepath, 'w', encoding='utf-8') as f:
        for feature in selected_features:
            f.write(feature + '\n')
        print(f"ğŸ”¹ selected_features íŒŒì¼ ì €ì¥: {features_filepath}")

    print(f'ğŸ”¹ Training {model_name}...')
    start_time = time.time()

    # k-fold ì‚¬ìš©í•˜ì—¬ í•™ìŠµìš© ë°ì´í„°ì…‹ ë‚˜ëˆ„ê¸°
    fold_count = 5
    kf = KFold(n_splits=fold_count, shuffle=True, random_state=seed)

    # ê° í•™ìŠµê²°ê³¼ ì €ì¥, í‰ê· ê°’ ì €ì¥í•  ì˜ˆì •
    rmse_scores = []
    submission_df = pd.DataFrame()

    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X)):
        print(f" - Fold {fold_idx+1} start")

        # ì˜ˆì¸¡ê°’ ì‹œê°í™”ë¥¼ ìœ„í•œ ì •ë³´ ì €ì¥
        result_bundle = {
            "model": model,
        }

        if is_submission:
            X_train = X.iloc[train_idx]
            y_train = y.iloc[train_idx]

            # í•™ìŠµ ì „ì²´ ë°ì´í„° train ìœ¼ë¡œ ì‚¬ìš©, test.csv ë¥¼ test ë¡œ ì‚¬ìš©
            model.fit(X_train, y_train)
            y_pred_log = model.predict(X_test)
            y_pred = np.expm1(y_pred_log)
            y_pred = np.round(y_pred).astype(int)  # ì •ìˆ˜ ë³€í™˜

            # ì œì¶œìš© DataFrame ìƒì„±
            submission_df[f'fold_{fold_idx+1}'] = y_pred

            result_bundle['y_pred'] = y_pred
        else:
            # í•™ìŠµ ë°ì´í„°ë¥¼ ë‚˜ëˆ ì„œ train, test ë¡œ ì‚¬ìš©
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            if model_name in 'XGB':
                model.fit(
                    X_train, 
                    y_train,
                    eval_set=[(X_val, y_val)],
                    early_stopping_rounds=100,
                    verbose=100
                )
            elif model_name in 'light':
                model.fit(
                    X_train, 
                    y_train,
                    eval_set=[(X_val, y_val)],
                    eval_metric="rmse",
                    callbacks=[
                        model.early_stopping(100)
                    ]
                )
            else:
                model.fit(X_train, y_train)

            y_pred_log = model.predict(X_val)
            y_pred = np.expm1(y_pred_log)  # ì˜ˆì¸¡ ë³µì›
            y_true = np.expm1(y_val)      # ì •ë‹µ ë³µì›

            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            rmse_scores.append(rmse)

            print(f'âœ… {model_name} fold {fold_idx} RMSE: {rmse:.2f}')

            result_bundle['y_pred'] = y_pred
            result_bundle['y_true'] = y_true
            result_bundle['rmse'] = rmse
            result_bundle['X_val'] = X_val

            # ë³€ìˆ˜ ì¤‘ìš”ë„ ì‹œê°í™” (ê°€ëŠ¥í•œ ê²½ìš°ë§Œ)
            if hasattr(model, 'feature_importances_'):
                show_feature_importances(model, model_name, selected_features)

        # í•˜ë‚˜ì˜ k-fold ëª¨ë¸ í•™ìŠµ ì¢…ë£Œ
        mode_str = 's' if is_submission else 't'
        output_filename = f"model_{model_name}_{fold_idx+1}_output_{mode_str}_yj.pkl"
        output_filepath = os.path.join(output_dir, output_filename)

        joblib.dump(result_bundle, output_filepath)
        print(f"{output_filename} íŒŒì¼ ì €ì¥: {output_filepath}")

    # ì „ì²´ k-fold í•™ìŠµ ì¢…ë£Œ
    end_time = time.time()
    print(f"í•™ìŠµ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")

    if is_submission:
        submission_df['target'] = submission_df[[f'fold_{i+1}' for i in range(kf.n_splits)]].mean(axis=1)
        final_submission_df = submission_df['target']

        output_filename = f"submission_{model_name}_yj.csv"
        output_filepath = os.path.join(output_dir, output_filename)

        # CSV íŒŒì¼ë¡œ ì €ì¥ (index=Falseë¡œ ì¸ë±ìŠ¤ ì œì™¸)
        final_submission_df.to_csv(output_filepath, index=False)
    else:
        rmse_avg = np.mean(rmse_scores)
    
    if is_submission:
        return 0
    else:
        return rmse_avg

# %%
xgb = XGBRegressor(
    n_estimators=3000,
    learning_rate=0.01,
    max_depth=12,
    min_child_weight=7,
    subsample=0.6,
    colsample_bytree=0.8,
    gamma=0, 
    reg_alpha=1,
    reg_lambda=1,
    random_state=seed,
    n_jobs=-1
)

model = xgb
model_name = 'XGBoost'

# # í•™ìŠµìš©
results = model_training(is_submission=False, 
                         model=model,
                         model_name=model_name, 
                         selected_features=selected_features, 
                         X=X,
                         y=y,
                         show_feature_importance=True)

# # ì œì¶œìš©
# results = model_training(is_submission=True, 
#                          model=model,
#                          model_name=model_name, 
#                          selected_features=selected_features, 
#                          X=X,
#                          y=y, 
#                          X_test=X_test)

print(results)

# %%
