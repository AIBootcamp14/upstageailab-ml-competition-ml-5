{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933c373d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger 모듈 로드 성공.\n",
      "2025-07-16 09:31:39 | ==================================================\n",
      "2025-07-16 09:31:39 | >> 아파트 가격 예측 모델링 시작\n",
      "2025-07-16 09:31:39 | >> [1-3단계 완료] 라이브러리, 경로, 로거 초기화 성공\n",
      "2025-07-16 09:31:39 | >> [4단계 시작] 데이터 로드 중...\n",
      "2025-07-16 09:31:42 | >> 데이터 로드 및 병합 완료.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 라이브러리 및 모듈 임포트 ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold # ✨ StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split # train_test_split\n",
    "import joblib\n",
    "import optuna\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "import matplotlib.font_manager as fm\n",
    "try:\n",
    "    font_path = '../../font/NanumFont/NanumGothic.ttf'\n",
    "    if os.path.exists(font_path):\n",
    "        fe = fm.FontEntry(fname=font_path, name='NanumGothic')\n",
    "        fm.fontManager.ttflist.insert(0, fe)\n",
    "        plt.rcParams.update({'font.size': 12, 'font.family': 'NanumGothic'})\n",
    "    else:\n",
    "        print(\"나눔고딕 폰트를 찾을 수 없어 기본 폰트로 설정됩니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"폰트 설정 중 오류 발생: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "# --- 2. 경로 설정 및 커스텀 로거 임포트 ---\n",
    "\n",
    "try:\n",
    "    src_path = os.path.abspath(os.path.join(os.getcwd(), \"../../src/log\"))\n",
    "    sys.path.insert(0, src_path)\n",
    "    from logger import Logger\n",
    "    print(\"Logger 모듈 로드 성공.\")\n",
    "except ImportError:\n",
    "    print(\"[오류] Logger 모듈을 찾을 수 없습니다.\")\n",
    "    class Logger:\n",
    "        def __init__(self, *args, **kwargs): pass\n",
    "        def write(self, message, **kwargs): print(message)\n",
    "        def start_redirect(self): pass\n",
    "        def stop_redirect(self): pass\n",
    "        def close(self): pass\n",
    "\n",
    "\n",
    "# --- 3. 로거 및 경로 초기화 ---\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "LOG_DIR = '../../data/logs/price_prediction_5_logs'\n",
    "LOG_FILENAME = f\"price_prediction_{timestamp}.log\"\n",
    "LOG_PATH = os.path.join(LOG_DIR, LOG_FILENAME)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "logger = Logger(log_path=LOG_PATH)\n",
    "\n",
    "TRAIN_PATH = '../../data/processed/cleaned_data/train_clean.csv'\n",
    "TEST_PATH = '../../data/processed/cleaned_data/test_clean.csv'\n",
    "\n",
    "SUBMISSION_DIR = '../../data/processed/submissions'\n",
    "SUBMISSION_FILENAME = 'price_prediction_5_submission.csv'\n",
    "SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, SUBMISSION_FILENAME)\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_DIR = '../../model'\n",
    "MODEL_FILENAME = 'price_prediction_5_model.pkl'\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "logger.write(\"=\"*50)\n",
    "logger.write(\">> 아파트 가격 예측 모델링 시작\")\n",
    "logger.write(\">> [1-3단계 완료] 라이브러리, 경로, 로거 초기화 성공\")\n",
    "\n",
    "\n",
    "# --- 4. 데이터 로드 ---\n",
    "\n",
    "logger.write(\">> [4단계 시작] 데이터 로드 중...\")\n",
    "try:\n",
    "    train_df_clean = pd.read_csv(TRAIN_PATH)\n",
    "    test_df_clean = pd.read_csv(TEST_PATH)\n",
    "    \n",
    "    train_df_clean['isTest'] = 0\n",
    "    test_df_clean['isTest'] = 1\n",
    "    \n",
    "    df = pd.concat([train_df_clean, test_df_clean])\n",
    "    logger.write(\">> 데이터 로드 및 병합 완료.\")\n",
    "except FileNotFoundError as e:\n",
    "    logger.write(f\">> [오류] 데이터 파일을 찾을 수 없습니다: {e}\", print_error=True)\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee978987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-16 09:31:42 | >> [5-1단계 시작] Target 변수 로그 변환을 시작합니다...\n",
      "2025-07-16 09:31:42 | >> Target 변수 로그 변환 완료.\n",
      "2025-07-16 09:31:42 | >> [5-2단계 시작] 고급 피처 엔지니어링 및 데이터 분리를 시작합니다...\n",
      "2025-07-16 09:31:42 | >> 1. Train/Test 데이터 분리 완료.\n",
      "2025-07-16 09:31:42 | >> 2. 시간 관련 피처 생성 완료.\n",
      "2025-07-16 09:31:43 | >> 3. 면적당 가격 관련 피처 생성 및 병합 완료.\n",
      "2025-07-16 09:31:43 | >> 4. 상호작용 특성 생성 완료.\n",
      "2025-07-16 09:31:43 | >> 5. 테스트 데이터 결측치 처리 완료.\n",
      "2025-07-16 09:31:43 | >> 6. 피처/타겟 정의 완료.\n",
      "2025-07-16 09:31:44 | >> 7. 범주형 피처 Label Encoding 완료.\n",
      "2025-07-16 09:31:44 | >> [5단계 완료] 모든 피처 엔지니어링 및 데이터 분리 성공.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 피처 엔지니어링 및 데이터 분리 ---\n",
    "\n",
    "# --- 5-1. 타겟 변환 (로그 변환) ---\n",
    "if 'df' in locals() and df is not None:\n",
    "    logger.write(\">> [5-1단계 시작] Target 변수 로그 변환을 시작합니다...\")\n",
    "    train_df = df[df['isTest'] == 0].copy()\n",
    "    train_df['target'] = np.log1p(train_df['target'])\n",
    "    df.loc[df['isTest']==0, 'target'] = train_df['target']\n",
    "    logger.write(\">> Target 변수 로그 변환 완료.\")\n",
    "\n",
    "\n",
    "# --- 5-2. 고급 피처 엔지니어링 ---\n",
    "if 'df' in locals() and df is not None:\n",
    "    try:\n",
    "        logger.write(\">> [5-2단계 시작] 고급 피처 엔지니어링 및 데이터 분리를 시작합니다...\")\n",
    "        \n",
    "        train_df = df[df['isTest'] == 0].copy()\n",
    "        test_df = df[df['isTest'] == 1].copy()\n",
    "        logger.write(\">> 1. Train/Test 데이터 분리 완료.\")\n",
    "\n",
    "        current_year = datetime.now().year\n",
    "        for temp_df in [train_df, test_df]:\n",
    "            temp_df['계약월_sin'] = np.sin(2 * np.pi * temp_df['계약월']/12)\n",
    "            temp_df['계약월_cos'] = np.cos(2 * np.pi * temp_df['계약월']/12)\n",
    "            temp_df['아파트나이'] = current_year - temp_df['연식']\n",
    "        logger.write(\">> 2. 시간 관련 피처 생성 완료.\")\n",
    "\n",
    "        train_df['면적당가격'] = train_df['target'] / train_df['전용면적']\n",
    "        \n",
    "        dong_price_stats = train_df.groupby('법정동')['면적당가격'].agg(['mean', 'std']).reset_index()\n",
    "        dong_price_stats.columns = ['법정동', '동별_평균면적당가격', '동별_면적당가격편차']\n",
    "        gu_price_stats = train_df.groupby('자치구')['면적당가격'].agg(['mean', 'std']).reset_index()\n",
    "        gu_price_stats.columns = ['자치구', '구별_평균면적당가격', '구별_면적당가격편차']\n",
    "        \n",
    "        train_df = pd.merge(train_df, dong_price_stats, on='법정동', how='left')\n",
    "        test_df = pd.merge(test_df, dong_price_stats, on='법정동', how='left')\n",
    "        train_df = pd.merge(train_df, gu_price_stats, on='자치구', how='left')\n",
    "        test_df = pd.merge(test_df, gu_price_stats, on='자치구', how='left')\n",
    "        logger.write(\">> 3. 면적당 가격 관련 피처 생성 및 병합 완료.\")\n",
    "        \n",
    "        for temp_df in [train_df, test_df]:\n",
    "            temp_df['면적_x_나이'] = temp_df['전용면적'] * temp_df['아파트나이']\n",
    "            temp_df['면적_x_층'] = temp_df['전용면적'] * temp_df['층']\n",
    "            temp_df['강남_x_면적'] = temp_df['강남3구여부'] * temp_df['전용면적']\n",
    "        logger.write(\">> 4. 상호작용 특성 생성 완료.\")\n",
    "        \n",
    "        fill_na_cols = ['동별_평균면적당가격', '동별_면적당가격편차', '구별_평균면적당가격', '구별_면적당가격편차']\n",
    "        mean_vals = train_df[fill_na_cols].mean()\n",
    "        test_df.fillna(mean_vals, inplace=True)\n",
    "        logger.write(\">> 5. 테스트 데이터 결측치 처리 완료.\")\n",
    "        \n",
    "        features = [col for col in train_df.columns if col not in ['target', 'id', '아파트이름', 'isTest', '면적당가격']]\n",
    "        \n",
    "        X_train_raw = train_df[features]\n",
    "        y_train = train_df['target']\n",
    "        X_test_raw = test_df[features]\n",
    "        logger.write(\">> 6. 피처/타겟 정의 완료.\")\n",
    "\n",
    "        categorical_features = X_train_raw.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        for col in categorical_features:\n",
    "            le = LabelEncoder()\n",
    "            all_vals = pd.concat([X_train_raw[col], X_test_raw[col]]).astype(str).unique()\n",
    "            le.fit(all_vals)\n",
    "            X_train_raw[col] = le.transform(X_train_raw[col].astype(str))\n",
    "            X_test_raw[col] = le.transform(X_test_raw[col].astype(str))\n",
    "            \n",
    "        logger.write(\">> 7. 범주형 피처 Label Encoding 완료.\")\n",
    "        logger.write(\">> [5단계 완료] 모든 피처 엔지니어링 및 데이터 분리 성공.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [오류] 피처 엔지니어링 중 심각한 문제 발생: {e}\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250bc010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-16 09:31:44 | >> [6단계 시작] 피처 선택을 위한 사전 모델 학습을 시작합니다...\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 4366\n",
      "[LightGBM] [Info] Number of data points in the train set: 1110101, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 10.706632\n",
      "2025-07-16 09:31:52 | >> 피처 선택 완료. 제외된 피처 개수: 10개\n",
      "2025-07-16 09:31:52 | >> 최종 학습에 사용될 피처 개수: 28개\n",
      "2025-07-16 09:31:52 | >> [6단계 완료] 피처 선택 성공.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. 피처 선택을 위한 사전 모델 학습 ---\n",
    "if 'X_train_raw' in locals():\n",
    "    logger.write(\">> [6단계 시작] 피처 선택을 위한 사전 모델 학습을 시작합니다...\")\n",
    "    try:\n",
    "        pre_model = lgb.LGBMRegressor(objective='regression_l1', metric='rmse', seed=42, device='cuda')\n",
    "        pre_model.fit(X_train_raw, y_train)\n",
    "        \n",
    "        feature_importances = pd.Series(pre_model.feature_importances_, index=X_train_raw.columns)\n",
    "        \n",
    "        threshold = 10 \n",
    "        low_importance_features = feature_importances[feature_importances <= threshold].index.tolist()\n",
    "        \n",
    "        X_train_selected = X_train_raw.drop(columns=low_importance_features)\n",
    "        X_test_selected = X_test_raw.drop(columns=low_importance_features)\n",
    "        \n",
    "        logger.write(f\">> 피처 선택 완료. 제외된 피처 개수: {len(low_importance_features)}개\")\n",
    "        logger.write(f\">> 최종 학습에 사용될 피처 개수: {X_train_selected.shape[1]}개\")\n",
    "        logger.write(\">> [6단계 완료] 피처 선택 성공.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [오류] 피처 선택 중 문제가 발생했습니다: {e}\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc335b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 09:31:52,834] A new study created in memory with name: no-name-e571b6dc-82b3-42e7-ab98-dffe1dc0d77e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-16 09:31:52 | >> [7단계 시작] Optuna로 하이퍼파라미터 최적화를 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 09:35:20,000] Trial 0 finished with value: 0.12960044491460482 and parameters: {'learning_rate': 0.014880925081744451, 'num_leaves': 105, 'max_depth': 22, 'subsample': 0.7671701740605588, 'colsample_bytree': 0.9191159837390863, 'lambda_l1': 0.020986620234181224, 'lambda_l2': 5.624712482981973e-05}. Best is trial 0 with value: 0.12960044491460482.\n",
      "[I 2025-07-16 09:39:24,639] Trial 1 finished with value: 0.11017596877475136 and parameters: {'learning_rate': 0.03784027818077158, 'num_leaves': 99, 'max_depth': 14, 'subsample': 0.7553238051109503, 'colsample_bytree': 0.7270672382305798, 'lambda_l1': 3.8915301153398134e-05, 'lambda_l2': 0.01831201399449299}. Best is trial 1 with value: 0.11017596877475136.\n",
      "[I 2025-07-16 09:42:59,164] Trial 2 finished with value: 0.10852059078305212 and parameters: {'learning_rate': 0.04301150119269317, 'num_leaves': 92, 'max_depth': 21, 'subsample': 0.7832815026515934, 'colsample_bytree': 0.7120832712215068, 'lambda_l1': 7.483684499072713, 'lambda_l2': 0.0032363805121359657}. Best is trial 2 with value: 0.10852059078305212.\n",
      "[I 2025-07-16 09:48:09,026] Trial 3 finished with value: 0.10281963545614399 and parameters: {'learning_rate': 0.039395753362756136, 'num_leaves': 138, 'max_depth': 20, 'subsample': 0.9907692560731882, 'colsample_bytree': 0.9192402177840291, 'lambda_l1': 3.163801832679002e-05, 'lambda_l2': 0.03559755257166951}. Best is trial 3 with value: 0.10281963545614399.\n",
      "[I 2025-07-16 09:51:59,377] Trial 4 finished with value: 0.10852105365148693 and parameters: {'learning_rate': 0.04477276715950412, 'num_leaves': 93, 'max_depth': 14, 'subsample': 0.7044879271244164, 'colsample_bytree': 0.8954695474517322, 'lambda_l1': 0.004537974031711173, 'lambda_l2': 0.0015819420550288242}. Best is trial 3 with value: 0.10281963545614399.\n",
      "[I 2025-07-16 09:54:17,135] Trial 5 finished with value: 0.1397416503420546 and parameters: {'learning_rate': 0.026805661705405943, 'num_leaves': 37, 'max_depth': 10, 'subsample': 0.7827299939544876, 'colsample_bytree': 0.8223970418261808, 'lambda_l1': 0.00018992193251673675, 'lambda_l2': 3.7745901255959774e-05}. Best is trial 3 with value: 0.10281963545614399.\n",
      "[I 2025-07-16 09:57:58,519] Trial 6 finished with value: 0.11067773581319056 and parameters: {'learning_rate': 0.03438483553067719, 'num_leaves': 102, 'max_depth': 24, 'subsample': 0.7455468599951056, 'colsample_bytree': 0.7596517341156169, 'lambda_l1': 5.117017049663049, 'lambda_l2': 1.8984915805523135e-05}. Best is trial 3 with value: 0.10281963545614399.\n",
      "[I 2025-07-16 09:59:56,541] Trial 7 finished with value: 0.14511004616595188 and parameters: {'learning_rate': 0.018414661909046733, 'num_leaves': 44, 'max_depth': 11, 'subsample': 0.733846214296651, 'colsample_bytree': 0.892613223490655, 'lambda_l1': 0.016033354638279783, 'lambda_l2': 2.2983041485191828e-05}. Best is trial 3 with value: 0.10281963545614399.\n",
      "[I 2025-07-16 10:03:48,393] Trial 8 finished with value: 0.1116621731869658 and parameters: {'learning_rate': 0.03120516394358875, 'num_leaves': 110, 'max_depth': 22, 'subsample': 0.9252240761186625, 'colsample_bytree': 0.9361519779591208, 'lambda_l1': 0.44573523790890124, 'lambda_l2': 1.3584745204974906}. Best is trial 3 with value: 0.10281963545614399.\n",
      "[I 2025-07-16 10:06:12,247] Trial 9 finished with value: 0.13213247997096889 and parameters: {'learning_rate': 0.026820518112574993, 'num_leaves': 50, 'max_depth': 23, 'subsample': 0.7590089059114117, 'colsample_bytree': 0.9014218080493082, 'lambda_l1': 0.9408099586814371, 'lambda_l2': 2.634417723566617e-05}. Best is trial 3 with value: 0.10281963545614399.\n",
      "[I 2025-07-16 10:10:52,321] Trial 10 finished with value: 0.09937549976866694 and parameters: {'learning_rate': 0.04551842838230966, 'num_leaves': 150, 'max_depth': 18, 'subsample': 0.9969226715045478, 'colsample_bytree': 0.9989883329182774, 'lambda_l1': 1.0884695596999528e-05, 'lambda_l2': 0.9329384432962574}. Best is trial 10 with value: 0.09937549976866694.\n",
      "[I 2025-07-16 10:15:56,687] Trial 11 finished with value: 0.09834885428146907 and parameters: {'learning_rate': 0.04921583376865926, 'num_leaves': 147, 'max_depth': 16, 'subsample': 0.9703779675772021, 'colsample_bytree': 0.9774137236694715, 'lambda_l1': 1.1239821246301837e-05, 'lambda_l2': 1.2015314027546453}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 10:20:44,806] Trial 12 finished with value: 0.09854746957826258 and parameters: {'learning_rate': 0.049766818660911386, 'num_leaves': 150, 'max_depth': 18, 'subsample': 0.9914587632943369, 'colsample_bytree': 0.988038560118554, 'lambda_l1': 1.0261103657040244e-05, 'lambda_l2': 6.9065973646078875}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 10:25:19,640] Trial 13 finished with value: 0.10140408526790214 and parameters: {'learning_rate': 0.04779058373914108, 'num_leaves': 128, 'max_depth': 17, 'subsample': 0.9099209786406941, 'colsample_bytree': 0.9977359857122939, 'lambda_l1': 0.0008249581575284811, 'lambda_l2': 8.593110097980272}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 10:29:45,712] Trial 14 finished with value: 0.1008477150344572 and parameters: {'learning_rate': 0.04964376622187347, 'num_leaves': 127, 'max_depth': 7, 'subsample': 0.9432701132696508, 'colsample_bytree': 0.9598487210377934, 'lambda_l1': 0.00038659816560007644, 'lambda_l2': 0.17878688178378455}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 10:32:51,002] Trial 15 finished with value: 0.11065137518245395 and parameters: {'learning_rate': 0.04983496800106519, 'num_leaves': 74, 'max_depth': 15, 'subsample': 0.8481730940993272, 'colsample_bytree': 0.8470089843248801, 'lambda_l1': 7.472209375051045e-05, 'lambda_l2': 9.84673562936545}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 10:37:52,442] Trial 16 finished with value: 0.10138968438242152 and parameters: {'learning_rate': 0.041505964410675444, 'num_leaves': 149, 'max_depth': 19, 'subsample': 0.8803965787713673, 'colsample_bytree': 0.9588005831802682, 'lambda_l1': 1.0765345603845138e-05, 'lambda_l2': 0.4210185475813962}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 10:40:48,491] Trial 17 finished with value: 0.11815297057162108 and parameters: {'learning_rate': 0.035293986889565726, 'num_leaves': 71, 'max_depth': 16, 'subsample': 0.9592466284289969, 'colsample_bytree': 0.8046201849332942, 'lambda_l1': 0.0014455764226622902, 'lambda_l2': 0.09976382865077744}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 10:44:36,443] Trial 18 finished with value: 0.11674968124240512 and parameters: {'learning_rate': 0.021705871572207713, 'num_leaves': 125, 'max_depth': 12, 'subsample': 0.8490434140554821, 'colsample_bytree': 0.959715092996054, 'lambda_l1': 0.00012139082785011142, 'lambda_l2': 3.3521087388851396}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 10:48:22,929] Trial 19 finished with value: 0.1302794350076213 and parameters: {'learning_rate': 0.011583608164939401, 'num_leaves': 135, 'max_depth': 25, 'subsample': 0.9690245068038863, 'colsample_bytree': 0.8653300833814506, 'lambda_l1': 0.0030682307451998466, 'lambda_l2': 2.0496765796680703}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 10:52:26,193] Trial 20 finished with value: 0.1026350854983506 and parameters: {'learning_rate': 0.04574315736166104, 'num_leaves': 121, 'max_depth': 18, 'subsample': 0.8986512281735788, 'colsample_bytree': 0.9741443631326049, 'lambda_l1': 0.08479040692385725, 'lambda_l2': 0.0003855904380000571}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 10:57:35,001] Trial 21 finished with value: 0.09952197872642092 and parameters: {'learning_rate': 0.046090177490989676, 'num_leaves': 147, 'max_depth': 17, 'subsample': 0.9977443714248819, 'colsample_bytree': 0.9990101856611425, 'lambda_l1': 1.238091627789342e-05, 'lambda_l2': 0.9522397065499912}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 11:02:24,361] Trial 22 finished with value: 0.1009035358888715 and parameters: {'learning_rate': 0.0424830744153405, 'num_leaves': 150, 'max_depth': 18, 'subsample': 0.9816668227789648, 'colsample_bytree': 0.9865184841158442, 'lambda_l1': 1.3776899776725368e-05, 'lambda_l2': 0.19267708358960225}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 11:07:23,888] Trial 23 finished with value: 0.10050923928914043 and parameters: {'learning_rate': 0.04670561984638877, 'num_leaves': 140, 'max_depth': 20, 'subsample': 0.9459194492062999, 'colsample_bytree': 0.941349592658552, 'lambda_l1': 4.667849178033247e-05, 'lambda_l2': 0.523166728805514}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 11:11:21,241] Trial 24 finished with value: 0.10642905269587234 and parameters: {'learning_rate': 0.039422393539104233, 'num_leaves': 115, 'max_depth': 16, 'subsample': 0.9986689985863858, 'colsample_bytree': 0.9753600564666266, 'lambda_l1': 0.00029094586230888253, 'lambda_l2': 2.9146662115094855}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 11:16:15,820] Trial 25 finished with value: 0.09934591473890086 and parameters: {'learning_rate': 0.04913227143402817, 'num_leaves': 136, 'max_depth': 13, 'subsample': 0.9364678332337159, 'colsample_bytree': 0.9403224770751749, 'lambda_l1': 4.776645471855837e-05, 'lambda_l2': 0.05170702090569715}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 11:20:47,933] Trial 26 finished with value: 0.09986571211076727 and parameters: {'learning_rate': 0.04974562737427032, 'num_leaves': 135, 'max_depth': 13, 'subsample': 0.8747514885433595, 'colsample_bytree': 0.9423539840691098, 'lambda_l1': 5.2958964932584875e-05, 'lambda_l2': 0.057894738824431004}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 11:25:08,526] Trial 27 finished with value: 0.10523329495077648 and parameters: {'learning_rate': 0.042153810816922495, 'num_leaves': 116, 'max_depth': 10, 'subsample': 0.9298642598721711, 'colsample_bytree': 0.8661661645912502, 'lambda_l1': 0.0006040558616296415, 'lambda_l2': 0.0074768431136748685}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 11:28:31,370] Trial 28 finished with value: 0.11493719069675805 and parameters: {'learning_rate': 0.0358554036072144, 'num_leaves': 81, 'max_depth': 14, 'subsample': 0.9658780343713471, 'colsample_bytree': 0.9248365112546444, 'lambda_l1': 0.00014019242348288963, 'lambda_l2': 0.3551560961308796}. Best is trial 11 with value: 0.09834885428146907.\n",
      "[I 2025-07-16 11:32:51,025] Trial 29 finished with value: 0.10645369451737567 and parameters: {'learning_rate': 0.031587853924295646, 'num_leaves': 141, 'max_depth': 8, 'subsample': 0.9490538842036271, 'colsample_bytree': 0.9649944801113087, 'lambda_l1': 0.07121586074520338, 'lambda_l2': 0.00022754795934036546}. Best is trial 11 with value: 0.09834885428146907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-16 11:32:51 | >> Optuna 탐색 완료. 최적 파라미터: {'learning_rate': 0.04921583376865926, 'num_leaves': 147, 'max_depth': 16, 'subsample': 0.9703779675772021, 'colsample_bytree': 0.9774137236694715, 'lambda_l1': 1.1239821246301837e-05, 'lambda_l2': 1.2015314027546453, 'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 2000, 'verbosity': -1, 'seed': 42, 'device': 'cuda'}\n",
      "2025-07-16 11:32:51 | >> [7단계 완료] 하이퍼파라미터 최적화 성공.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. 하이퍼파라미터 최적화 (Optuna) ---\n",
    "\n",
    "# Optuna 외부에서 훈련/검증 데이터 한 번만 분리\n",
    "# StratifiedKFold를 위해 y_train을 binned 값으로 사용\n",
    "num_bins = 10\n",
    "y_binned_for_split = pd.cut(y_train, bins=num_bins, labels=False, include_lowest=True)\n",
    "X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "    X_train_selected, y_train, test_size=0.2, random_state=42, stratify=y_binned_for_split\n",
    ")\n",
    "\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "    param = {\n",
    "        'objective': 'regression_l1', 'metric': 'rmse', \n",
    "        'n_estimators': 1000,  # [수정 2] n_estimators 값 감소\n",
    "        'verbosity': -1, 'boosting_type': 'gbdt', 'seed': 42,\n",
    "        'device': 'cuda', 'n_jobs': -1,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 30, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 7, 25),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-5, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-5, 10.0, log=True),\n",
    "    }\n",
    "\n",
    "    # K-Fold 반복문 제거, 단일 모델로 학습 및 평가\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_metric='rmse',\n",
    "              callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    \n",
    "    val_preds = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "if 'X_train_selected' in locals():\n",
    "    logger.write(\">> [7단계 시작] Optuna로 하이퍼파라미터 최적화를 시작합니다...\")\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    # 분리된 데이터를 objective 함수에 전달\n",
    "    # n_trials 값을 30 정도로 조절하여 1시간 내외로 실행\n",
    "    study.optimize(lambda trial: objective(trial, X_train_opt, y_train_opt, X_val_opt, y_val_opt), n_trials=30) \n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_params.update({\n",
    "        'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 2000, # 최종 모델은 n_estimators 늘려서 학습\n",
    "        'verbosity': -1, 'seed': 42, 'device': 'cuda'\n",
    "    })\n",
    "    logger.write(f\">> Optuna 탐색 완료. 최적 파라미터: {best_params}\")\n",
    "    logger.write(\">> [7단계 완료] 하이퍼파라미터 최적화 성공.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d824617",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 8. 최종 모델 학습 (Stratified K-Fold) ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mX_train_selected\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mbest_params\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_redirect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m         logger.write(\u001b[33m\"\u001b[39m\u001b[33m>> [8단계 시작] 최적 파라미터로 Stratified K-Fold 교차 검증 및 모델 학습을 시작합니다...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Lab/upstageailab-ml-competition-ml-5/src/log/logger.py:52\u001b[39m, in \u001b[36mLogger.start_redirect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart_redirect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     49\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03m    표준 출력(stdout)과 표준 에러(stderr)를 이 로거 인스턴스로 리디렉션합니다.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m>> 표준 출력 및 오류를 로그 파일로 리디렉션 시작\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_also\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     sys.stdout = \u001b[38;5;28mself\u001b[39m\n\u001b[32m     54\u001b[39m     sys.stderr = \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI_Lab/upstageailab-ml-competition-ml-5/src/log/logger.py:33\u001b[39m, in \u001b[36mLogger.write\u001b[39m\u001b[34m(self, message, print_also, print_error)\u001b[39m\n\u001b[32m     30\u001b[39m timestamp = datetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     31\u001b[39m line = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28mself\u001b[39m.log_file.write(line)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.print_also \u001b[38;5;129;01mand\u001b[39;00m print_also:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m print_error:\n\u001b[32m     37\u001b[39m         \u001b[38;5;66;03m# 재귀 호출을 피하기 위해 원본 표준 출력을 사용합니다.\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "# --- 8. 최종 모델 학습 (Stratified K-Fold) ---\n",
    "if 'X_train_selected' in locals() and 'best_params' in locals():\n",
    "    logger.start_redirect()\n",
    "    try:\n",
    "        logger.write(\">> [8단계 시작] 최적 파라미터로 Stratified K-Fold 교차 검증 및 모델 학습을 시작합니다...\")\n",
    "        \n",
    "        num_bins = 10\n",
    "        y_binned = pd.cut(y_train, bins=num_bins, labels=False, include_lowest=True)\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        oof_preds = np.zeros(X_train_selected.shape[0])\n",
    "        test_preds = np.zeros(X_test_selected.shape[0])\n",
    "        rmse_scores = []\n",
    "        models = []\n",
    "        logger.write(\">> K-Fold 설정 완료. 5-Fold 교차 검증을 시작합니다.\")\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_selected, y_binned)):\n",
    "            logger.write(f\"--- [Fold {fold+1}/5] 학습 시작 ---\")\n",
    "            X_train_fold, X_val_fold = X_train_selected.iloc[train_idx], X_train_selected.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            model = lgb.LGBMRegressor(**best_params)\n",
    "            model.fit(X_train_fold, y_train_fold,\n",
    "                      eval_set=[(X_val_fold, y_val_fold)],\n",
    "                      eval_metric='rmse',\n",
    "                      callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "            val_preds = model.predict(X_val_fold)\n",
    "            oof_preds[val_idx] = val_preds\n",
    "            \n",
    "            rmse = np.sqrt(mean_squared_error(y_val_fold, val_preds))\n",
    "            rmse_scores.append(rmse)\n",
    "            models.append(model)\n",
    "            logger.write(f\"✅ Fold {fold+1} Log-RMSE: {rmse:.4f}\")\n",
    "            \n",
    "            test_preds += np.expm1(model.predict(X_test_selected)) / skf.get_n_splits()\n",
    "\n",
    "        avg_rmse = np.mean(rmse_scores)\n",
    "        logger.write(\"-------------------------------------------\")\n",
    "        logger.write(f\"✅ 최종 CV 평균 Log-RMSE: {avg_rmse:.4f}\")\n",
    "        logger.write(\"-------------------------------------------\")\n",
    "        \n",
    "        logger.write(\">> [8단계 완료] 모든 모델 학습 및 평가 성공.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [오류] 모델 학습 중 심각한 문제 발생: {e}\", print_error=True)\n",
    "    finally:\n",
    "        logger.stop_redirect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f97dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-16 11:32:51 | >> [9단계 시작] 모델 결과 시각화 및 분석을 시작합니다...\n",
      "2025-07-16 11:32:51 | >> 1. 피처 중요도 시각화 중...\n",
      "\u001b[91m2025-07-16 11:32:51 | >> [오류] 시각화 및 분석 중 문제가 발생했습니다: 'feature'\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# --- 9. 최종 모델 결과 시각화 및 분석 ---\n",
    "if 'models' in locals() and 'oof_preds' in locals():\n",
    "    logger.write(\">> [9단계 시작] 모델 결과 시각화 및 분석을 시작합니다...\")\n",
    "    try:\n",
    "        # 1. 피처 중요도 시각화\n",
    "        logger.write(\">> 1. 피처 중요도 시각화 중...\")\n",
    "        feature_importances = pd.DataFrame()\n",
    "        for i, model in enumerate(models):\n",
    "            fold_importance = pd.DataFrame({\n",
    "                'feature': X_train_selected.columns,\n",
    "                'importance': model.feature_importances_,\n",
    "                'fold': i + 1\n",
    "            })\n",
    "            feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\n",
    "        \n",
    "        mean_importances = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.barplot(x=mean_importances.head(20).values, y=mean_importances.head(20).index)\n",
    "        plt.title('상위 20개 피처 중요도 (평균)', fontsize=16)\n",
    "        plt.show()\n",
    "        logger.write(\">> 피처 중요도 시각화 완료.\")\n",
    "\n",
    "        # 2. 실제 값 vs OOF 예측 값 비교\n",
    "        logger.write(\">> 2. 실제 값 vs OOF 예측 값 비교 시각화 중...\")\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        sns.scatterplot(x=np.expm1(y_train), y=np.expm1(oof_preds), alpha=0.3)\n",
    "        plt.plot([np.expm1(y_train).min(), np.expm1(y_train).max()], [np.expm1(y_train).min(), np.expm1(y_train).max()], 'r--', lw=2)\n",
    "        plt.title('실제 값 vs OOF 예측 값 비교', fontsize=16)\n",
    "        plt.show()\n",
    "        logger.write(\">> 실제 값 vs OOF 예측 값 비교 시각화 완료.\")\n",
    "\n",
    "        # 3. 잔차 분포 확인\n",
    "        logger.write(\">> 3. 잔차 분포 확인 시각화 중...\")\n",
    "        residuals = np.expm1(y_train) - np.expm1(oof_preds)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(residuals, kde=True, bins=50)\n",
    "        plt.title('잔차(실제-예측) 분포 (OOF 기반)', fontsize=16)\n",
    "        plt.show()\n",
    "        logger.write(\">> 잔차 분포 확인 시각화 완료.\")\n",
    "        \n",
    "        # 4. 학습/테스트 데이터 예측 분포 비교\n",
    "        logger.write(\">> 4. 학습/테스트 데이터 예측 분포 비교 시각화 중...\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.kdeplot(np.expm1(y_train), label='학습 데이터 실제 값', fill=True, alpha=0.5)\n",
    "        sns.kdeplot(test_preds, label='테스트 데이터 예측 값', fill=True, alpha=0.5)\n",
    "        plt.title('학습 데이터와 테스트 예측의 분포 비교', fontsize=16)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        logger.write(\">> 학습/테스트 데이터 예측 분포 비교 시각화 완료.\")\n",
    "\n",
    "        # 5. SHAP 분석\n",
    "        logger.write(\">> 5. SHAP 분석 시작 (계산에 시간이 다소 소요될 수 있습니다)...\")\n",
    "        explainer = shap.TreeExplainer(models[0])\n",
    "        shap_sample = X_train_selected.sample(1000, random_state=42)\n",
    "        shap_values = explainer.shap_values(shap_sample)\n",
    "\n",
    "        logger.write(\">> SHAP 요약 플롯 생성 중...\")\n",
    "        shap.summary_plot(shap_values, shap_sample, plot_type=\"dot\", show=False)\n",
    "        plt.title(\"SHAP 요약 플롯 (첫 번째 폴드 모델)\", fontsize=16)\n",
    "        plt.show()\n",
    "        logger.write(\">> SHAP 분석 완료.\")\n",
    "        \n",
    "        logger.write(\">> [9단계 완료] 시각화 및 분석 완료.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [오류] 시각화 및 분석 중 문제가 발생했습니다: {e}\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacdaefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-16 11:32:51 | >> [10단계 시작] 최종 제출 파일 생성을 시작합니다...\n",
      "2025-07-16 11:32:51 | >> 전체 훈련 데이터로 최종 모델 학습 시작...\n",
      "\u001b[91m2025-07-16 11:32:51 | >> [오류] 제출 파일 생성 중 문제 발생: lightgbm.sklearn.LGBMRegressor() got multiple values for keyword argument 'device'\n",
      "\u001b[0m2025-07-16 11:32:51 | >> 모델링 종료\n",
      "2025-07-16 11:32:51 | ==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 10. 최종 예측 및 제출 파일 생성 ---\n",
    "if 'test_preds' in locals():\n",
    "    logger.write(\">> [10단계 시작] 최종 제출 파일 생성을 시작합니다...\")\n",
    "    try:\n",
    "        # 최종 모델로 전체 데이터 학습\n",
    "        logger.write(\">> 전체 훈련 데이터로 최종 모델 학습 시작...\")\n",
    "        final_model = lgb.LGBMRegressor(device='cuda', **best_params)\n",
    "        final_model.fit(X_train_selected, y_train)\n",
    "        logger.write(\">> 최종 모델 학습 완료.\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        joblib.dump(final_model, MODEL_PATH)\n",
    "        logger.write(f\">> 모델 저장 완료: {MODEL_PATH}\")\n",
    "\n",
    "        # 제출 파일 생성\n",
    "        logger.write(\">> 'target' 컬럼만 포함된 제출 파일을 생성합니다.\")\n",
    "        submission_df = pd.DataFrame({'target': test_preds})\n",
    "        submission_df['target'] = submission_df['target'].astype(int)\n",
    "        \n",
    "        submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "        logger.write(f\">> 제출 파일 생성 완료: {SUBMISSION_PATH}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [오류] 제출 파일 생성 중 문제 발생: {e}\", print_error=True)\n",
    "    \n",
    "    logger.write(\">> 모델링 종료\")\n",
    "    logger.write(\"=\"*50 + \"\\n\")\n",
    "    #logger.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house_price_predict_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
