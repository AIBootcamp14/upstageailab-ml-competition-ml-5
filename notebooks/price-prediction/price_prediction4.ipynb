{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "933c373d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger 모듈 로드 성공.\n",
      "2025-07-16 01:57:11 | ==================================================\n",
      "2025-07-16 01:57:11 | >> 아파트 가격 예측 모델링 시작\n",
      "2025-07-16 01:57:11 | >> [1-3단계 완료] 라이브러리, 경로, 로거 초기화 성공\n",
      "2025-07-16 01:57:11 | >> [4단계 시작] 데이터 로드 중...\n",
      "2025-07-16 01:57:12 | >> 데이터 로드 및 병합 완료.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 라이브러리 및 모듈 임포트 ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold # ✨ (수정) StratifiedKFold 임포트\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import optuna\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "import matplotlib.font_manager as fm\n",
    "try:\n",
    "    font_path = '../../font/NanumFont/NanumGothic.ttf'\n",
    "    if os.path.exists(font_path):\n",
    "        fe = fm.FontEntry(fname=font_path, name='NanumGothic')\n",
    "        fm.fontManager.ttflist.insert(0, fe)\n",
    "        plt.rcParams.update({'font.size': 12, 'font.family': 'NanumGothic'})\n",
    "    else:\n",
    "        print(\"나눔고딕 폰트를 찾을 수 없어 기본 폰트로 설정됩니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"폰트 설정 중 오류 발생: {e}\")\n",
    "    pass\n",
    "\n",
    "\n",
    "# --- 2. 경로 설정 및 커스텀 로거 임포트 ---\n",
    "try:\n",
    "    src_path = os.path.abspath(os.path.join(os.getcwd(), \"../../src/log\"))\n",
    "    sys.path.insert(0, src_path)\n",
    "    from logger import Logger\n",
    "    print(\"Logger 모듈 로드 성공.\")\n",
    "except ImportError:\n",
    "    print(\"[오류] Logger 모듈을 찾을 수 없습니다.\")\n",
    "    class Logger:\n",
    "        def __init__(self, *args, **kwargs): pass\n",
    "        def write(self, message, **kwargs): print(message)\n",
    "        def start_redirect(self): pass\n",
    "        def stop_redirect(self): pass\n",
    "        def close(self): pass\n",
    "\n",
    "\n",
    "# --- 3. 로거 및 경로 초기화 ---\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "LOG_DIR = '../../data/logs/price_prediction_4_logs'\n",
    "LOG_FILENAME = f\"price_prediction_{timestamp}.log\"\n",
    "LOG_PATH = os.path.join(LOG_DIR, LOG_FILENAME)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "logger = Logger(log_path=LOG_PATH)\n",
    "\n",
    "TRAIN_PATH = '../../data/processed/cleaned_data/train_clean.csv'\n",
    "TEST_PATH = '../../data/processed/cleaned_data/test_clean.csv'\n",
    "\n",
    "SUBMISSION_DIR = '../../data/processed/submissions'\n",
    "SUBMISSION_FILENAME = 'price_prediction_4_submission.csv'\n",
    "SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, SUBMISSION_FILENAME)\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_DIR = '../../model'\n",
    "MODEL_FILENAME = 'price_prediction_4_model.pkl'\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "logger.write(\"=\"*50)\n",
    "logger.write(\">> 아파트 가격 예측 모델링 시작\")\n",
    "logger.write(\">> [1-3단계 완료] 라이브러리, 경로, 로거 초기화 성공\")\n",
    "\n",
    "\n",
    "# --- 4. 데이터 로드 ---\n",
    "logger.write(\">> [4단계 시작] 데이터 로드 중...\")\n",
    "try:\n",
    "    train_df_clean = pd.read_csv(TRAIN_PATH)\n",
    "    test_df_clean = pd.read_csv(TEST_PATH)\n",
    "    \n",
    "    train_df_clean['isTest'] = 0\n",
    "    test_df_clean['isTest'] = 1\n",
    "    \n",
    "    df = pd.concat([train_df_clean, test_df_clean])\n",
    "    logger.write(\">> 데이터 로드 및 병합 완료.\")\n",
    "except FileNotFoundError as e:\n",
    "    logger.write(f\">> [오류] 데이터 파일을 찾을 수 없습니다: {e}\", print_error=True)\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee978987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-16 01:57:12 | >> [5-1단계 시작] Target 변수 로그 변환을 시작합니다...\n",
      "2025-07-16 01:57:13 | >> Target 변수 로그 변환 완료.\n",
      "2025-07-16 01:57:13 | >> [5-2단계 시작] 고급 피처 엔지니어링 및 데이터 분리를 시작합니다...\n",
      "2025-07-16 01:57:13 | >> 1. Train/Test 데이터 분리 완료.\n",
      "2025-07-16 01:57:13 | >> 2. 시간 관련 피처 생성 완료.\n",
      "2025-07-16 01:57:13 | >> 3. 면적당 가격 관련 피처 생성 및 병합 완료.\n",
      "2025-07-16 01:57:13 | >> 4. 상호작용 특성 생성 완료.\n",
      "2025-07-16 01:57:13 | >> 5. 테스트 데이터 결측치 처리 완료.\n",
      "2025-07-16 01:57:14 | >> 6. 피처/타겟 정의 완료.\n",
      "2025-07-16 01:57:14 | >> 7. 범주형 피처 Label Encoding 완료.\n",
      "2025-07-16 01:57:14 | >> [5단계 완료] 모든 피처 엔지니어링 및 데이터 분리 성공.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 피처 엔지니어링 및 데이터 분리 ---\n",
    "\n",
    "# --- 5-1. 타겟 변환 (로그 변환) ---\n",
    "if 'df' in locals() and df is not None:\n",
    "    logger.write(\">> [5-1단계 시작] Target 변수 로그 변환을 시작합니다...\")\n",
    "    train_df = df[df['isTest'] == 0].copy()\n",
    "    train_df['target'] = np.log1p(train_df['target'])\n",
    "    df.loc[df['isTest']==0, 'target'] = train_df['target']\n",
    "    logger.write(\">> Target 변수 로그 변환 완료.\")\n",
    "\n",
    "\n",
    "# --- 5-2. 고급 피처 엔지니어링 ---\n",
    "if 'df' in locals() and df is not None:\n",
    "    try:\n",
    "        logger.write(\">> [5-2단계 시작] 고급 피처 엔지니어링 및 데이터 분리를 시작합니다...\")\n",
    "        \n",
    "        train_df = df[df['isTest'] == 0].copy()\n",
    "        test_df = df[df['isTest'] == 1].copy()\n",
    "        logger.write(\">> 1. Train/Test 데이터 분리 완료.\")\n",
    "\n",
    "        current_year = datetime.now().year\n",
    "        for temp_df in [train_df, test_df]:\n",
    "            temp_df['계약월_sin'] = np.sin(2 * np.pi * temp_df['계약월']/12)\n",
    "            temp_df['계약월_cos'] = np.cos(2 * np.pi * temp_df['계약월']/12)\n",
    "            temp_df['아파트나이'] = current_year - temp_df['연식']\n",
    "        logger.write(\">> 2. 시간 관련 피처 생성 완료.\")\n",
    "\n",
    "        train_df['면적당가격'] = train_df['target'] / train_df['전용면적']\n",
    "        \n",
    "        dong_price_stats = train_df.groupby('법정동')['면적당가격'].agg(['mean', 'std']).reset_index()\n",
    "        dong_price_stats.columns = ['법정동', '동별_평균면적당가격', '동별_면적당가격편차']\n",
    "        gu_price_stats = train_df.groupby('자치구')['면적당가격'].agg(['mean', 'std']).reset_index()\n",
    "        gu_price_stats.columns = ['자치구', '구별_평균면적당가격', '구별_면적당가격편차']\n",
    "        \n",
    "        train_df = pd.merge(train_df, dong_price_stats, on='법정동', how='left')\n",
    "        test_df = pd.merge(test_df, dong_price_stats, on='법정동', how='left')\n",
    "        train_df = pd.merge(train_df, gu_price_stats, on='자치구', how='left')\n",
    "        test_df = pd.merge(test_df, gu_price_stats, on='자치구', how='left')\n",
    "        logger.write(\">> 3. 면적당 가격 관련 피처 생성 및 병합 완료.\")\n",
    "        \n",
    "        for temp_df in [train_df, test_df]:\n",
    "            temp_df['면적_x_나이'] = temp_df['전용면적'] * temp_df['아파트나이']\n",
    "            temp_df['면적_x_층'] = temp_df['전용면적'] * temp_df['층']\n",
    "            temp_df['강남_x_면적'] = temp_df['강남3구여부'] * temp_df['전용면적']\n",
    "        logger.write(\">> 4. 상호작용 특성 생성 완료.\")\n",
    "        \n",
    "        fill_na_cols = ['동별_평균면적당가격', '동별_면적당가격편차', '구별_평균면적당가격', '구별_면적당가격편차']\n",
    "        mean_vals = train_df[fill_na_cols].mean()\n",
    "        test_df.fillna(mean_vals, inplace=True)\n",
    "        logger.write(\">> 5. 테스트 데이터 결측치 처리 완료.\")\n",
    "        \n",
    "        features = [col for col in train_df.columns if col not in ['target', 'id', '아파트이름', 'isTest', '면적당가격']]\n",
    "        \n",
    "        X_train_raw = train_df[features]\n",
    "        y_train = train_df['target']\n",
    "        X_test_raw = test_df[features]\n",
    "        logger.write(\">> 6. 피처/타겟 정의 완료.\")\n",
    "\n",
    "        categorical_features = X_train_raw.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        for col in categorical_features:\n",
    "            le = LabelEncoder()\n",
    "            all_vals = pd.concat([X_train_raw[col], X_test_raw[col]]).astype(str).unique()\n",
    "            le.fit(all_vals)\n",
    "            X_train_raw[col] = le.transform(X_train_raw[col].astype(str))\n",
    "            X_test_raw[col] = le.transform(X_test_raw[col].astype(str))\n",
    "            \n",
    "        logger.write(\">> 7. 범주형 피처 Label Encoding 완료.\")\n",
    "        logger.write(\">> [5단계 완료] 모든 피처 엔지니어링 및 데이터 분리 성공.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [오류] 피처 엔지니어링 중 심각한 문제 발생: {e}\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "250bc010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-16 01:57:14 | >> [6단계 시작] 피처 선택을 위한 사전 모델 학습을 시작합니다...\n",
      "2025-07-16 01:57:18 | >> 피처 선택 완료. 제외된 피처 개수: 10개\n",
      "2025-07-16 01:57:18 | >> 최종 학습에 사용될 피처 개수: 28개\n",
      "2025-07-16 01:57:18 | >> [6단계 완료] 피처 선택 성공.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. 피처 선택을 위한 사전 모델 학습 ---\n",
    "if 'X_train_raw' in locals():\n",
    "    logger.write(\">> [6단계 시작] 피처 선택을 위한 사전 모델 학습을 시작합니다...\")\n",
    "    try:\n",
    "        pre_model = lgb.LGBMRegressor(objective='regression_l1', metric='rmse', seed=42, device='cuda')\n",
    "        pre_model.fit(X_train_raw, y_train)\n",
    "        \n",
    "        feature_importances = pd.Series(pre_model.feature_importances_, index=X_train_raw.columns)\n",
    "        \n",
    "        threshold = 10 \n",
    "        low_importance_features = feature_importances[feature_importances <= threshold].index.tolist()\n",
    "        \n",
    "        X_train_selected = X_train_raw.drop(columns=low_importance_features)\n",
    "        X_test_selected = X_test_raw.drop(columns=low_importance_features)\n",
    "        \n",
    "        logger.write(f\">> 피처 선택 완료. 제외된 피처 개수: {len(low_importance_features)}개\")\n",
    "        logger.write(f\">> 최종 학습에 사용될 피처 개수: {X_train_selected.shape[1]}개\")\n",
    "        logger.write(\">> [6단계 완료] 피처 선택 성공.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [오류] 피처 선택 중 문제가 발생했습니다: {e}\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc335b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 01:57:18,683] A new study created in memory with name: no-name-952fbc25-8f74-441a-9847-307f416bf562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-16 01:57:18 | >> [7단계 시작] Optuna로 하이퍼파라미터 최적화를 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 02:15:47,235] Trial 0 finished with value: 0.09543521301789627 and parameters: {'learning_rate': 0.030500446227644085, 'num_leaves': 134, 'max_depth': 23, 'subsample': 0.9497086681928294, 'colsample_bytree': 0.9383430577854994, 'lambda_l1': 3.8655527717661093, 'lambda_l2': 0.7929069278424327}. Best is trial 0 with value: 0.09543521301789627.\n",
      "[I 2025-07-16 02:28:30,648] Trial 1 finished with value: 0.12012046029397386 and parameters: {'learning_rate': 0.0271765974007228, 'num_leaves': 40, 'max_depth': 25, 'subsample': 0.8600102841466462, 'colsample_bytree': 0.923670706486452, 'lambda_l1': 0.0002276832767407633, 'lambda_l2': 8.995896314333048e-05}. Best is trial 0 with value: 0.09543521301789627.\n",
      "[I 2025-07-16 02:42:33,907] Trial 2 finished with value: 0.10792409287634883 and parameters: {'learning_rate': 0.04977508218851048, 'num_leaves': 39, 'max_depth': 11, 'subsample': 0.7090726040027178, 'colsample_bytree': 0.7371059220075789, 'lambda_l1': 6.589703151742541e-05, 'lambda_l2': 0.0622580219267443}. Best is trial 0 with value: 0.09543521301789627.\n",
      "[I 2025-07-16 02:58:28,617] Trial 3 finished with value: 0.10196606307184689 and parameters: {'learning_rate': 0.03213565881238162, 'num_leaves': 85, 'max_depth': 19, 'subsample': 0.7341688344597567, 'colsample_bytree': 0.9081437029293545, 'lambda_l1': 0.004476926791520387, 'lambda_l2': 0.28710610643579143}. Best is trial 0 with value: 0.09543521301789627.\n",
      "[I 2025-07-16 03:13:21,364] Trial 4 finished with value: 0.10413208530791887 and parameters: {'learning_rate': 0.0420673801504842, 'num_leaves': 57, 'max_depth': 19, 'subsample': 0.9356751935346047, 'colsample_bytree': 0.9464728301506047, 'lambda_l1': 1.0222619318438684e-05, 'lambda_l2': 3.183521305151399e-05}. Best is trial 0 with value: 0.09543521301789627.\n",
      "[I 2025-07-16 03:33:22,632] Trial 5 finished with value: 0.09030750285500058 and parameters: {'learning_rate': 0.044467154943652096, 'num_leaves': 136, 'max_depth': 23, 'subsample': 0.8666049401070772, 'colsample_bytree': 0.9259067701883383, 'lambda_l1': 8.320362140878745e-05, 'lambda_l2': 0.006343585824492134}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 03:50:24,894] Trial 6 finished with value: 0.10235733450699422 and parameters: {'learning_rate': 0.021159556095216477, 'num_leaves': 128, 'max_depth': 13, 'subsample': 0.7999247159054514, 'colsample_bytree': 0.9026164912870813, 'lambda_l1': 0.2595360902457639, 'lambda_l2': 0.0021265533110929502}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 04:05:27,147] Trial 7 finished with value: 0.1057113805074327 and parameters: {'learning_rate': 0.025543057273473363, 'num_leaves': 86, 'max_depth': 15, 'subsample': 0.7050523201156234, 'colsample_bytree': 0.8047747899210339, 'lambda_l1': 3.0978538125825303, 'lambda_l2': 0.0008083533931589933}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 04:19:10,185] Trial 8 finished with value: 0.11119112034062906 and parameters: {'learning_rate': 0.043127003320492574, 'num_leaves': 38, 'max_depth': 23, 'subsample': 0.8544460946094842, 'colsample_bytree': 0.9659143753071386, 'lambda_l1': 0.0032530653200710537, 'lambda_l2': 0.9190293445149968}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 04:33:07,651] Trial 9 finished with value: 0.10791624405500277 and parameters: {'learning_rate': 0.04255466490848486, 'num_leaves': 45, 'max_depth': 14, 'subsample': 0.7026736476295585, 'colsample_bytree': 0.7745296859623303, 'lambda_l1': 1.8847644502932877, 'lambda_l2': 0.11681030233031019}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 04:48:19,398] Trial 10 finished with value: 0.11330270535921241 and parameters: {'learning_rate': 0.010567735671961283, 'num_leaves': 146, 'max_depth': 9, 'subsample': 0.9985878718953581, 'colsample_bytree': 0.8504843930788155, 'lambda_l1': 0.05468516722218292, 'lambda_l2': 0.012028836963817844}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 05:06:29,962] Trial 11 finished with value: 0.09595414092306381 and parameters: {'learning_rate': 0.0343171998700707, 'num_leaves': 119, 'max_depth': 21, 'subsample': 0.9244486128630499, 'colsample_bytree': 0.9920465538525789, 'lambda_l1': 0.0004292827948107049, 'lambda_l2': 8.459928316652373}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 05:21:21,091] Trial 12 finished with value: 0.11087050732722263 and parameters: {'learning_rate': 0.01543512090884602, 'num_leaves': 112, 'max_depth': 25, 'subsample': 0.9170953785185474, 'colsample_bytree': 0.8678484801283906, 'lambda_l1': 0.037927687673443886, 'lambda_l2': 5.124749660542965}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 05:41:08,770] Trial 13 finished with value: 0.09161533334633873 and parameters: {'learning_rate': 0.035903286656241686, 'num_leaves': 150, 'max_depth': 20, 'subsample': 0.9727974646815126, 'colsample_bytree': 0.9918921572047033, 'lambda_l1': 8.901447342313432, 'lambda_l2': 0.011929340030443802}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 06:01:23,422] Trial 14 finished with value: 0.09197551640219154 and parameters: {'learning_rate': 0.03688886223927313, 'num_leaves': 147, 'max_depth': 18, 'subsample': 0.802876168059387, 'colsample_bytree': 0.9931478891030032, 'lambda_l1': 0.2890166900539985, 'lambda_l2': 0.008552528641174204}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 06:19:38,301] Trial 15 finished with value: 0.09239846130114375 and parameters: {'learning_rate': 0.04972451566232889, 'num_leaves': 104, 'max_depth': 21, 'subsample': 0.9896317135733266, 'colsample_bytree': 0.8817255127541554, 'lambda_l1': 1.3061496585838321e-05, 'lambda_l2': 0.0016958520287025814}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 06:39:14,406] Trial 16 finished with value: 0.0925479463473844 and parameters: {'learning_rate': 0.03750965998995711, 'num_leaves': 136, 'max_depth': 17, 'subsample': 0.8980729642121403, 'colsample_bytree': 0.9645023181168381, 'lambda_l1': 0.0025698999357052048, 'lambda_l2': 0.0002545606423035424}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 06:57:06,685] Trial 17 finished with value: 0.09355436332631648 and parameters: {'learning_rate': 0.04543958149492085, 'num_leaves': 102, 'max_depth': 21, 'subsample': 0.7795726947634286, 'colsample_bytree': 0.8201952620900181, 'lambda_l1': 0.00043874687511858925, 'lambda_l2': 0.01924827968578781}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 07:12:13,681] Trial 18 finished with value: 0.10122875973064879 and parameters: {'learning_rate': 0.038107160704635344, 'num_leaves': 73, 'max_depth': 22, 'subsample': 0.8762555136413129, 'colsample_bytree': 0.7051403867851139, 'lambda_l1': 0.02176956042310205, 'lambda_l2': 0.0068111127454249494}. Best is trial 5 with value: 0.09030750285500058.\n",
      "[I 2025-07-16 07:32:04,475] Trial 19 finished with value: 0.08928679412886863 and parameters: {'learning_rate': 0.04552644898504375, 'num_leaves': 148, 'max_depth': 16, 'subsample': 0.9496338786876218, 'colsample_bytree': 0.9743654033592622, 'lambda_l1': 0.31530673959591266, 'lambda_l2': 0.00037532302299416777}. Best is trial 19 with value: 0.08928679412886863.\n",
      "[I 2025-07-16 07:50:44,321] Trial 20 finished with value: 0.09097757590915055 and parameters: {'learning_rate': 0.046506123437466566, 'num_leaves': 125, 'max_depth': 7, 'subsample': 0.817256769268802, 'colsample_bytree': 0.8876476790191885, 'lambda_l1': 0.27167887551126846, 'lambda_l2': 0.00028589340725788514}. Best is trial 19 with value: 0.08928679412886863.\n",
      "[I 2025-07-16 08:09:23,706] Trial 21 finished with value: 0.09039110180284347 and parameters: {'learning_rate': 0.04751206572052117, 'num_leaves': 125, 'max_depth': 7, 'subsample': 0.8250710165499059, 'colsample_bytree': 0.887736685421015, 'lambda_l1': 0.34586873700719206, 'lambda_l2': 0.00039525472460557297}. Best is trial 19 with value: 0.08928679412886863.\n",
      "[I 2025-07-16 08:28:44,515] Trial 22 finished with value: 0.08952727099954265 and parameters: {'learning_rate': 0.04643830807741123, 'num_leaves': 138, 'max_depth': 12, 'subsample': 0.827345308962136, 'colsample_bytree': 0.9579825356695989, 'lambda_l1': 0.7440833950492447, 'lambda_l2': 1.2524629994749525e-05}. Best is trial 19 with value: 0.08928679412886863.\n",
      "[I 2025-07-16 08:47:41,731] Trial 23 finished with value: 0.09132329342310311 and parameters: {'learning_rate': 0.04031775418851117, 'num_leaves': 137, 'max_depth': 12, 'subsample': 0.888806018544413, 'colsample_bytree': 0.9657163482833492, 'lambda_l1': 1.1971871116576351, 'lambda_l2': 1.921824007970639e-05}. Best is trial 19 with value: 0.08928679412886863.\n",
      "[I 2025-07-16 09:05:37,707] Trial 24 finished with value: 0.09255623441257932 and parameters: {'learning_rate': 0.045000806494711595, 'num_leaves': 113, 'max_depth': 15, 'subsample': 0.7543458483796281, 'colsample_bytree': 0.9404848435666057, 'lambda_l1': 0.10746636534064893, 'lambda_l2': 6.892608568482056e-05}. Best is trial 19 with value: 0.08928679412886863.\n",
      "[I 2025-07-16 09:24:49,494] Trial 25 finished with value: 0.09092820006871323 and parameters: {'learning_rate': 0.041134340475341126, 'num_leaves': 140, 'max_depth': 10, 'subsample': 0.8323616138477218, 'colsample_bytree': 0.9157389577521766, 'lambda_l1': 0.010171566582702886, 'lambda_l2': 1.1621878632286345e-05}. Best is trial 19 with value: 0.08928679412886863.\n",
      "[I 2025-07-16 09:57:02,230] Trial 26 finished with value: 0.09397826945728359 and parameters: {'learning_rate': 0.046714953947171414, 'num_leaves': 98, 'max_depth': 17, 'subsample': 0.9593997634449434, 'colsample_bytree': 0.9686344545516088, 'lambda_l1': 0.8611259992907002, 'lambda_l2': 5.781026636261393e-05}. Best is trial 19 with value: 0.08928679412886863.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. 하이퍼파라미터 최적화 (Optuna) ---\n",
    "def objective(trial, X, y):\n",
    "    param = {\n",
    "        'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 2000,\n",
    "        'verbosity': -1, 'boosting_type': 'gbdt', 'seed': 42,\n",
    "        'n_jobs': -1,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 30, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 7, 25),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-5, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-5, 10.0, log=True),\n",
    "    }\n",
    "\n",
    "    num_bins = 10\n",
    "    y_binned = pd.cut(y, bins=num_bins, labels=False, include_lowest=True)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_binned)):\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = lgb.LGBMRegressor(device='cuda', **param)\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                  eval_set=[(X_val_fold, y_val_fold)],\n",
    "                  eval_metric='rmse',\n",
    "                  callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "        \n",
    "        val_preds = model.predict(X_val_fold)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_fold, val_preds))\n",
    "        rmse_scores.append(rmse)\n",
    "        \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "if 'X_train_selected' in locals():\n",
    "    logger.write(\">> [7단계 시작] Optuna로 하이퍼파라미터 최적화를 시작합니다...\")\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train_selected, y_train), n_trials=50) \n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_params.update({\n",
    "        'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 2000,\n",
    "        'verbosity': -1, 'seed': 42\n",
    "    })\n",
    "    logger.write(f\">> Optuna 탐색 완료. 최적 파라미터: {best_params}\")\n",
    "    logger.write(\">> [7단계 완료] 하이퍼파라미터 최적화 성공.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d824617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. 최종 모델 학습 (Stratified K-Fold) ---\n",
    "if 'X_train_selected' in locals() and 'best_params' in locals():\n",
    "    logger.start_redirect()\n",
    "    try:\n",
    "        logger.write(\">> [8단계 시작] 최적 파라미터로 Stratified K-Fold 교차 검증 및 모델 학습을 시작합니다...\")\n",
    "        \n",
    "        num_bins = 10\n",
    "        y_binned = pd.cut(y_train, bins=num_bins, labels=False, include_lowest=True)\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        oof_preds = np.zeros(X_train_selected.shape[0])\n",
    "        test_preds = np.zeros(X_test_selected.shape[0])\n",
    "        rmse_scores = []\n",
    "        models = []\n",
    "        logger.write(\">> K-Fold 설정 완료. 5-Fold 교차 검증을 시작합니다.\")\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_selected, y_binned)):\n",
    "            logger.write(f\"--- [Fold {fold+1}/5] 학습 시작 ---\")\n",
    "            X_train_fold, X_val_fold = X_train_selected.iloc[train_idx], X_train_selected.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            model = lgb.LGBMRegressor(device='cuda', **best_params)\n",
    "            model.fit(X_train_fold, y_train_fold,\n",
    "                      eval_set=[(X_val_fold, y_val_fold)],\n",
    "                      eval_metric='rmse',\n",
    "                      callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "            val_preds = model.predict(X_val_fold)\n",
    "            oof_preds[val_idx] = val_preds\n",
    "            \n",
    "            rmse = np.sqrt(mean_squared_error(y_val_fold, val_preds))\n",
    "            rmse_scores.append(rmse)\n",
    "            models.append(model)\n",
    "            logger.write(f\"✅ Fold {fold+1} Log-RMSE: {rmse:.4f}\")\n",
    "            \n",
    "            test_preds += np.expm1(model.predict(X_test_selected)) / skf.get_n_splits()\n",
    "\n",
    "        avg_rmse = np.mean(rmse_scores)\n",
    "        logger.write(\"-------------------------------------------\")\n",
    "        logger.write(f\"✅ 최종 CV 평균 Log-RMSE: {avg_rmse:.4f}\")\n",
    "        logger.write(\"-------------------------------------------\")\n",
    "        \n",
    "        logger.write(\">> [8단계 완료] 모든 모델 학습 및 평가 성공.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [오류] 모델 학습 중 심각한 문제 발생: {e}\", print_error=True)\n",
    "    finally:\n",
    "        logger.stop_redirect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f97dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. 최종 모델 결과 시각화 및 분석 ---\n",
    "if 'models' in locals() and 'oof_preds' in locals():\n",
    "    logger.write(\">> [9단계 시작] 모델 결과 시각화 및 분석을 시작합니다...\")\n",
    "    try:\n",
    "        # 1. 피처 중요도 시각화\n",
    "        logger.write(\">> 1. 피처 중요도 시각화 중...\")\n",
    "        feature_importances = pd.DataFrame()\n",
    "        for i, model in enumerate(models):\n",
    "            fold_importance = pd.DataFrame({\n",
    "                'feature': X_train_selected.columns,\n",
    "                'importance': model.feature_importances_,\n",
    "                'fold': i + 1\n",
    "            })\n",
    "            feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\n",
    "        \n",
    "        mean_importances = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.barplot(x=mean_importances.head(20).values, y=mean_importances.head(20).index)\n",
    "        plt.title('상위 20개 피처 중요도 (평균)', fontsize=16)\n",
    "        plt.show()\n",
    "        logger.write(\">> 피처 중요도 시각화 완료.\")\n",
    "\n",
    "        # 2. 실제 값 vs OOF 예측 값 비교\n",
    "        logger.write(\">> 2. 실제 값 vs OOF 예측 값 비교 시각화 중...\")\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        sns.scatterplot(x=np.expm1(y_train), y=np.expm1(oof_preds), alpha=0.3)\n",
    "        plt.plot([np.expm1(y_train).min(), np.expm1(y_train).max()], [np.expm1(y_train).min(), np.expm1(y_train).max()], 'r--', lw=2)\n",
    "        plt.title('실제 값 vs OOF 예측 값 비교', fontsize=16)\n",
    "        plt.show()\n",
    "        logger.write(\">> 실제 값 vs OOF 예측 값 비교 시각화 완료.\")\n",
    "\n",
    "        # 3. 잔차 분포 확인\n",
    "        logger.write(\">> 3. 잔차 분포 확인 시각화 중...\")\n",
    "        residuals = np.expm1(y_train) - np.expm1(oof_preds)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(residuals, kde=True, bins=50)\n",
    "        plt.title('잔차(실제-예측) 분포 (OOF 기반)', fontsize=16)\n",
    "        plt.show()\n",
    "        logger.write(\">> 잔차 분포 확인 시각화 완료.\")\n",
    "        \n",
    "        # 4. 학습/테스트 데이터 예측 분포 비교\n",
    "        logger.write(\">> 4. 학습/테스트 데이터 예측 분포 비교 시각화 중...\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.kdeplot(np.expm1(y_train), label='학습 데이터 실제 값', fill=True, alpha=0.5)\n",
    "        sns.kdeplot(test_preds, label='테스트 데이터 예측 값', fill=True, alpha=0.5)\n",
    "        plt.title('학습 데이터와 테스트 예측의 분포 비교', fontsize=16)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        logger.write(\">> 학습/테스트 데이터 예측 분포 비교 시각화 완료.\")\n",
    "\n",
    "        # 5. SHAP 분석\n",
    "        logger.write(\">> 5. SHAP 분석 시작 (계산에 시간이 다소 소요될 수 있습니다)...\")\n",
    "        explainer = shap.TreeExplainer(models[0])\n",
    "        shap_sample = X_train_selected.sample(1000, random_state=42)\n",
    "        shap_values = explainer.shap_values(shap_sample)\n",
    "\n",
    "        logger.write(\">> SHAP 요약 플롯 생성 중...\")\n",
    "        shap.summary_plot(shap_values, shap_sample, plot_type=\"dot\", show=False)\n",
    "        plt.title(\"SHAP 요약 플롯 (첫 번째 폴드 모델)\", fontsize=16)\n",
    "        plt.show()\n",
    "        logger.write(\">> SHAP 분석 완료.\")\n",
    "        \n",
    "        logger.write(\">> [9단계 완료] 시각화 및 분석 완료.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [오류] 시각화 및 분석 중 문제가 발생했습니다: {e}\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacdaefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 10. 최종 예측 및 제출 파일 생성 ---\n",
    "if 'test_preds' in locals():\n",
    "    logger.write(\">> [10단계 시작] 최종 제출 파일 생성을 시작합니다...\")\n",
    "    try:\n",
    "        # 최종 모델로 전체 데이터 학습\n",
    "        logger.write(\">> 전체 훈련 데이터로 최종 모델 학습 시작...\")\n",
    "        final_model = lgb.LGBMRegressor(device='cuda', **best_params)\n",
    "        final_model.fit(X_train_selected, y_train)\n",
    "        logger.write(\">> 최종 모델 학습 완료.\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        joblib.dump(final_model, MODEL_PATH)\n",
    "        logger.write(f\">> 모델 저장 완료: {MODEL_PATH}\")\n",
    "\n",
    "        # 제출 파일 생성\n",
    "        logger.write(\">> 'target' 컬럼만 포함된 제출 파일을 생성합니다.\")\n",
    "        submission_df = pd.DataFrame({'target': test_preds})\n",
    "        submission_df['target'] = submission_df['target'].astype(int)\n",
    "        \n",
    "        submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "        logger.write(f\">> 제출 파일 생성 완료: {SUBMISSION_PATH}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [오류] 제출 파일 생성 중 문제 발생: {e}\", print_error=True)\n",
    "    \n",
    "    logger.write(\">> 모델링 종료\")\n",
    "    logger.write(\"=\"*50 + \"\\n\")\n",
    "    logger.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house_price_predict_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
